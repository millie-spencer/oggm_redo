{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff2b5d5-26ab-4850-aa0a-81a52d3f031e",
   "metadata": {},
   "source": [
    "# Climate projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f495ab8-501b-4124-b425-974d224a0ed1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# climate-related\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintake\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgcsfs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcftime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'intake'"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import os \n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "# climate-related\n",
    "import intake\n",
    "import gcsfs\n",
    "import cftime\n",
    "from xclim import core \n",
    "from xclim import sdba\n",
    "from xclim import set_options\n",
    "\n",
    "# others\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#bug in specific gcm\n",
    "#from xmip.preprocessing import combined_preprocessing\n",
    "\n",
    "chunks_dict = {\"lon\": 10, \"lat\": 10, \"time\": -1}\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") \n",
    "\n",
    "# check what gcm to use (not only generic list)\n",
    "gcm_list  = [\"ACCESS-CM2\", \"BCC-CSM2-MR\", \"CMCC-ESM2\", \"FGOALS-f3-L\", \"GFDL-ESM4\", \"CMCC-CM2-SR5\", \"KACE-1-0-G\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\", \"MIROC6\"]\n",
    "ssp_list  = [\"ssp126\",\"ssp245\",\"ssp370\", \"ssp585\"] \n",
    "climate   = [\"ERA5\", \"MSWEP\", \"PMET\", \"CR2MET\"]\n",
    "bias_correction = [\"MVA\",\"DQM\",\"MBC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53dc91-9e70-4574-921b-7ceee184c7c3",
   "metadata": {},
   "source": [
    "## 1. Download and preprocess selected GCMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb6be2-9a88-48d3-9977-d31eecc6b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "dataframe = intake.open_esm_datastore(url)\n",
    "\n",
    "dataframe = dataframe.search(experiment_id = ['historical'] + ssp_list, # scenarios  \n",
    "                             table_id      = 'Amon', # time-step\n",
    "                             variable_id   = ['tas', 'pr'], # variables\n",
    "                             member_id     = 'r1i1p1f1', # member\n",
    "                             source_id     = gcm_list)  # models\n",
    "\n",
    "datasets = dataframe.to_dataset_dict()\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf469a4-929f-45f4-a20c-9d77d824bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = slice(\"1980-01-01\", \"2099-12-25\") # period\n",
    "days = np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])\n",
    "\n",
    "lat_coords = np.arange(-56,-40, 0.5)\n",
    "lon_coords = np.arange(-76,-67, 0.5)\n",
    "\n",
    "for gcm in tqdm(gcm_list):\n",
    "    gcm_historical  = next(val for key, val in datasets.items() if gcm + \".historical\" in key)\n",
    "    \n",
    "    if isinstance(gcm_historical.indexes['time'][0], cftime.DatetimeProlepticGregorian) or isinstance(gcm_historical.indexes['time'][0], cftime.Datetime360Day) or isinstance(gcm_historical.indexes['time'][0], cftime.DatetimeNoLeap):\n",
    "        gcm_historical[\"time\"] = gcm_historical.indexes['time'].to_datetimeindex() # fix the time dimension if its necessary\n",
    "    \n",
    "    for ssp in tqdm(ssp_list):\n",
    "        \n",
    "        gcm_ssp = next(val for key, val in datasets.items() if gcm + \".\" + ssp in key)\n",
    "        gcm_ssp = gcm_ssp.sel(time=period) # fix for 2100-2300\n",
    "        \n",
    "        if isinstance(gcm_ssp.indexes['time'][0], cftime.DatetimeProlepticGregorian) or isinstance(gcm_ssp.indexes['time'][0], cftime.Datetime360Day) or isinstance(gcm_ssp.indexes['time'][0], cftime.DatetimeNoLeap):\n",
    "            gcm_ssp[\"time\"] = gcm_ssp.indexes['time'].to_datetimeindex() # fix the time dimension if its necessary\n",
    "        \n",
    "        gcm_ssp = xr.concat([gcm_historical, gcm_ssp], dim = \"time\")\n",
    "        gcm_ssp = gcm_ssp.drop([\"height\", \"dcpp_init_year\", \"member_id\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\"])\n",
    "        gcm_ssp = gcm_ssp.sel(member_id=0, drop=True).sel(dcpp_init_year=0, drop=True)\n",
    "        gcm_ssp.coords['lon'] = (gcm_ssp.coords['lon'] + 180) % 360 - 180\n",
    "        gcm_ssp = gcm_ssp.sortby(gcm_ssp.lon)\n",
    "        gcm_ssp = gcm_ssp.interp(lat = lat_coords, lon = lon_coords)\n",
    "        gcm_ssp = gcm_ssp.sel(time=period)\n",
    "\n",
    "        # air temperature in K\n",
    "        gcm_ssp.tas.to_netcdf(\"/home/rooda/OGGM_results/Future_climate/T2M_\" + gcm + \"_\" + ssp + \".nc\")\n",
    "        \n",
    "        # precipitation flux in kg m-2 s-1\n",
    "        gcm_ssp.pr.to_netcdf(\"/home/rooda/OGGM_results/Future_climate/PP_\"   + gcm + \"_\" + ssp + \".nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dbaba-7872-4f6f-8ecc-c1c7e0f9f389",
   "metadata": {},
   "source": [
    "## 2. Bias corrections methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24468a-1551-44ac-ad31-39ae87731a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(ref, hist, future, clip): \n",
    "    hist_mean = hist.groupby('time.month').mean(dim='time')\n",
    "    ref_mean  = ref.groupby('time.month').mean(dim='time')\n",
    "    \n",
    "    hist_sd = hist.groupby('time.month').std(dim='time')\n",
    "    ref_sd  = ref.groupby('time.month').std(dim='time')\n",
    "    \n",
    "    future_bc = future.groupby('time.month') - hist_mean\n",
    "    future_bc = future_bc.groupby('time.month') * (ref_sd / hist_sd)\n",
    "    future_bc = future_bc.groupby('time.month') + ref_mean\n",
    "    \n",
    "    if clip:\n",
    "        future_bc = future_bc.clip(min = 0)\n",
    "        \n",
    "    return future_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e8aaa-d87e-4ff9-b5b4-4f5b0ee8a4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks_dict = {\"lon\": 20, \"lat\": 20, \"time\": -1}\n",
    "encode_t2m  = {'tas': {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_pp   = {\"pr\": {\"zlib\": True, \"complevel\": 1, \"dtype\": \"float32\"}}\n",
    "\n",
    "for file_id in tqdm(climate): \n",
    "    \n",
    "    future_period   = slice(\"2020-01-01\", \"2099-12-31\") # Future period to bias correct\n",
    "    baseline_period = slice(\"1980-01-01\", \"2014-12-31\") # ISIMIP3b bias adjustment protocol\n",
    "\n",
    "    pp_baseline  = xr.open_dataset(\"/home/rooda/OGGM_results/\" + file_id + \"_OGGM_1980_2019m.nc\")[\"prcp\"]\n",
    "    pp_baseline  = pp_baseline.sel(time  = baseline_period)\n",
    "    pp_baseline.attrs['units']   = \"mm month-1\"\n",
    "    pp_baseline = core.units.convert_units_to(pp_baseline, target = 'kg m-2 s-1', context = \"hydro\")\n",
    "    \n",
    "    t2m_baseline = xr.open_dataset(\"/home/rooda/OGGM_results/\" + file_id + \"_OGGM_1980_2019m.nc\")[\"temp\"]\n",
    "    t2m_baseline = t2m_baseline.sel(time = baseline_period)\n",
    "    t2m_baseline.attrs['units']   = \"C\"\n",
    "    t2m_baseline = core.units.convert_units_to(t2m_baseline, target = 'K')\n",
    "    \n",
    "    os.chdir(\"/home/rooda/OGGM_results/Future_climate/\")\n",
    "\n",
    "    for gcm in tqdm(gcm_list, leave=True):\n",
    "        for ssp in tqdm(ssp_list, leave = False):\n",
    "            pp_model_ssp = xr.open_dataset(\"PP_\" + gcm + \"_\" + ssp + \".nc\")[\"pr\"]\n",
    "            pp_model_ssp = pp_model_ssp.interp(lat = pp_baseline.lat, lon = pp_baseline.lon)\n",
    "            pp_model_ssp = pp_model_ssp.where(pp_baseline[0].notnull())\n",
    "            pp_model_ssp = pp_model_ssp.chunk(chunks_dict)\n",
    "\n",
    "            t2m_model_ssp = xr.open_mfdataset(\"T2M_\" + gcm + \"_\" + ssp + \".nc\")[\"tas\"]\n",
    "            t2m_model_ssp = t2m_model_ssp.interp(lat = t2m_baseline.lat, lon = t2m_baseline.lon)\n",
    "            t2m_model_ssp = t2m_model_ssp.where(t2m_baseline[0].notnull())\n",
    "            t2m_model_ssp = t2m_model_ssp.chunk(chunks_dict)\n",
    "            \n",
    "            for bc in bias_correction: \n",
    "                if bc == \"MVA\": # Mean and variance scaling (faster in memory)\n",
    "                    t2m_model_ssp_bc  = scaling(ref = t2m_baseline.load(), \n",
    "                                                hist = t2m_model_ssp.sel(time  = baseline_period).load(), \n",
    "                                                future = t2m_model_ssp.sel(time  = future_period).load(),\n",
    "                                                clip = False) \n",
    "\n",
    "                    pp_model_ssp_bc  = scaling(ref = pp_baseline.load(), \n",
    "                                               hist = pp_model_ssp.sel(time  = baseline_period).load(), \n",
    "                                               future = pp_model_ssp.sel(time  = future_period).load(), \n",
    "                                               clip = True) \n",
    "    \n",
    "                if bc == \"DQM\": # Quantile Delta Mapping method\n",
    "                    qdm_t2m = sdba.adjustment.QuantileDeltaMapping.train(ref = t2m_baseline, hist = t2m_model_ssp.sel(time = baseline_period), kind = \"+\", group=\"time.month\")\n",
    "                    t2m_model_ssp_bc = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period), interp=\"nearest\", extrapolation=\"constant\")      \n",
    "\n",
    "                    qdm_pp  = sdba.adjustment.QuantileDeltaMapping.train(ref = pp_baseline, hist = pp_model_ssp.sel(time  = baseline_period), kind = \"*\", group=\"time.month\")\n",
    "                    pp_model_ssp_bc  = qdm_pp.adjust(pp_model_ssp.sel(time  = future_period), interp=\"nearest\", extrapolation=\"constant\")\n",
    "\n",
    "                if bc == \"MBC\": # Npdf Transform method (xclim.readthedocs.io/en/stable/notebooks/sdba.html)\n",
    "                    \n",
    "                    ## a) Perform an initial univariate adjustment\n",
    "                    qdm_t2m = sdba.QuantileDeltaMapping.train(t2m_baseline, t2m_model_ssp.sel(time = baseline_period), nquantiles = 20, kind = \"+\", group = \"time\")\n",
    "                    scen_hist_t2m = qdm_t2m.adjust(t2m_model_ssp.sel(time = baseline_period))\n",
    "                    scen_ssp_t2m  = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period))\n",
    "                    \n",
    "                    qdm_pp = sdba.QuantileDeltaMapping.train(pp_baseline, pp_model_ssp.sel(time = baseline_period), nquantiles = 20, kind = \"*\", group = \"time\")\n",
    "                    scen_hist_pp = qdm_pp.adjust(pp_model_ssp.sel(time = baseline_period))\n",
    "                    scen_ssp_pp  = qdm_pp.adjust(pp_model_ssp.sel(time = future_period))\n",
    "\n",
    "                    dref      = xr.Dataset(dict(tas = t2m_baseline, pr = pp_baseline))\n",
    "                    scen_hist = xr.Dataset(dict(tas = scen_hist_t2m, pr = scen_hist_pp))\n",
    "                    scen_ssp  = xr.Dataset(dict(tas = scen_ssp_t2m, pr = scen_ssp_pp))\n",
    "                    scen_hist[\"time\"] = dref.time # correct date (15 -> 01)\n",
    "                    \n",
    "                    ## b) Stack the variables to multivariate arrays and standardize them\n",
    "                    ref   = sdba.processing.stack_variables(dref) # Stack the variables (tas and pr)\n",
    "                    scenh = sdba.processing.stack_variables(scen_hist)\n",
    "                    scens = sdba.processing.stack_variables(scen_ssp)\n",
    "            \n",
    "                    ref, _, _          = sdba.processing.standardize(ref) # Standardize\n",
    "                    allsim, savg, sstd = sdba.processing.standardize(xr.concat((scenh, scens), \"time\"))\n",
    "\n",
    "                    hist = allsim.sel(time = scenh.time)\n",
    "                    sim  = allsim.sel(time = scens.time)\n",
    "                                        \n",
    "                    ## c) Perform the N-dimensional probability density function transform\n",
    "                    out = sdba.adjustment.NpdfTransform.adjust(ref, hist, sim, base=sdba.QuantileDeltaMapping, \n",
    "                                                               base_kws={\"nquantiles\": 20, \"group\": \"time.month\"}, n_iter=20, n_escore=1000)  \n",
    "                    model_ssp_bc = sdba.processing.reordering(out, scens, group=\"time.month\")\n",
    "                    model_ssp_bc = sdba.processing.unstack_variables(model_ssp_bc)\n",
    "                    \n",
    "                    ## d) Restoring the trend\n",
    "                    model_ssp_bc = sdba.processing.reordering(sim, scens, group=\"time\")\n",
    "                    model_ssp_bc = sdba.processing.unstack_variables(model_ssp_bc)\n",
    "\n",
    "                    t2m_model_ssp_bc = model_ssp_bc.tas\n",
    "                    pp_model_ssp_bc  = model_ssp_bc.pr.clip(min = 0)\n",
    "                    \n",
    "                # save file\n",
    "                rid = \"_{}_{}_{}_{}\".format(file_id, gcm, ssp, bc)\n",
    "\n",
    "                t2m_model_ssp_bc = t2m_model_ssp_bc.rename(\"tas\").transpose('time', 'lat', 'lon')\n",
    "                t2m_model_ssp_bc.to_netcdf(\"/home/rooda/OGGM_results/Future_climate_bc/T2M\" + rid + \".nc\", encoding = encode_t2m)\n",
    "                pp_model_ssp_bc  = pp_model_ssp_bc.rename(\"pr\").transpose('time', 'lat', 'lon')\n",
    "                pp_model_ssp_bc.to_netcdf(\"/home/rooda/OGGM_results/Future_climate_bc/PP\" + rid + \".nc\", encoding = encode_pp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
